{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final quiz, 3 of 3\n",
    "\n",
    "## Decision Trees, Random Forests\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "Step through this jupyter notebook and write code to complete the instructed tasks.  \n",
    "Questions in the notebook are mirrored in the quiz on Canvas. Submit answers to the  \n",
    "questions on Canvas. When you are done, restart your notebook and run all the cells.  \n",
    "Save your notebook **without clearing the output**. Download your saved notebook  \n",
    "(if you are working on jupyterhub rather than locally) and submit the file on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll import that tools you'll need.\n",
    "# You may import additional tools if you wish to.\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit recognition\n",
    "\n",
    "In our neural network exercise, we trained a NN for classifying images of handwritten digits. We'll do the same here, using trees and forests. Our image sizes will be smaller than those we used for the NN exercise, to allow for faster processing. Images are 8x8 pixels, which are flattened into 64 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 of 3\n",
    "\n",
    "### Train a decision tree classifier\n",
    "\n",
    "The cell below will load the data set. Add code to the cell and run it, to help you answer the following questions.\n",
    "\n",
    "**Q4: How many samples are in the data set?**\n",
    "\n",
    "---\n",
    "Split your data into 80% training and 20% testing sets. **In your call to ```train_test_split()```, set the random_state parameter to 0, so you will get the same result as everyone else: ```random_state=0```.**\n",
    "\n",
    "Construct and train a decision tree model on the data. Let the tree grow to have maximum depth (```max_depth=None```). **Again, set the random_state parameter to 0 (```random_state=0```).** (Although the training is largely deterministic, sklearn randomly selects a feature if two or more features would give equal Gini Impurity reduction at a split).\n",
    "\n",
    "Train the tree on your training set and test it on the test set.\n",
    "\n",
    "**Q5: What is the decision tree accuracy score for the test set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8472222222222222"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################\n",
    "## Part 1 of 2\n",
    "##################\n",
    "## Load the (flattened) digit images and labels\n",
    "# X is a numpy array of pixels, with a shape of (num_samples, num_features)\n",
    "# y is a numpy array of labels (digit numbers), with a shape of (num_samples)\n",
    "(X, y) = datasets.load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "# Show your work below. Submit your answers on Canvas.\n",
    "# You may create additional cells to segment your work.\n",
    "#######################################################\n",
    "len(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)\n",
    "\n",
    "photo = DecisionTreeClassifier(max_depth=None, random_state=0)\n",
    "photo.fit(X_train, y_train)\n",
    "\n",
    "acc_score = photo.score(X_test, y_test)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 of 3\n",
    "\n",
    "### Train several random forest models\n",
    "\n",
    "Add code to the cell below and run it, to help you answer the following questions.\n",
    "\n",
    "---\n",
    "**Model 1**:\n",
    "Using the same training and test sets you created above, construct, train, and test a random forest classifier with the following parameters:\n",
    "```\n",
    "n_estimators=10\n",
    "bootstrap=True\n",
    "max_features='sqrt'\n",
    "random_state=0\n",
    "```\n",
    "This creates a model that is a traditional RF model, including sample bagging (```bootstrap=True```) and feature _subset_ randomization (```max_features='sqrt'```).\n",
    "\n",
    "**Q6: What is the RF Model 1 accuracy score for the test set?**\n",
    "\n",
    "\n",
    "---\n",
    "**Model 2**:\n",
    "Using the same training and test sets you created above, construct, train, and test a random forest classifier with the following parameters:\n",
    "```\n",
    "n_estimators=10\n",
    "bootstrap=False\n",
    "max_features=64\n",
    "random_state=0\n",
    "```\n",
    "This creates a model that **does not** randomly select subsets of samples and features. All the trees in the forest are nearly identical (the only difference being the random selection amongst feature that give equal reductions in Gini Impurity).\n",
    "\n",
    "**Q7: What is the RF Model 2 accuracy score for the test set?**\n",
    "\n",
    "\n",
    "---\n",
    "**Model 3**:\n",
    "Using the same training and test sets you created above, construct, train, and test a random forest classifier with the following parameters:\n",
    "```\n",
    "n_estimators=100\n",
    "bootstrap=True\n",
    "max_features='sqrt'\n",
    "random_state=0\n",
    "```\n",
    "This is the same as Model 1 (a traditional RF model), except you'll use 100 trees rather than 10 trees. More trees often results in a better model.\n",
    "\n",
    "**Q8: What is the RF Model 3 accuracy score for the test set?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "## Part 2 of 3\n",
    "##################\n",
    "#\n",
    "# Show your work below. Submit your answers on Canvas.\n",
    "# You may create additional cells to segment your work.\n",
    "\n",
    "# Method 1\n",
    "M1 = RandomForestClassifier(n_estimators=10, bootstrap=True, max_features='sqrt', random_state=0)\n",
    "M1.fit(X_train, y_train)\n",
    "\n",
    "M1_Score = M1.score(X_test, y_test)\n",
    "print(M1_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8833333333333333\n"
     ]
    }
   ],
   "source": [
    "# Method 2\n",
    "M2 = RandomForestClassifier(n_estimators=10, bootstrap=False, max_features=64, random_state=0)\n",
    "M2.fit(X_train, y_train)\n",
    "\n",
    "M2_Score = M2.score(X_test, y_test)\n",
    "print(M2_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9694444444444444\n"
     ]
    }
   ],
   "source": [
    "# Method 3\n",
    "M3 = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features='sqrt', random_state=0)\n",
    "M3.fit(X_train, y_train)\n",
    "\n",
    "M3_Score = M3.score(X_test, y_test)\n",
    "print(M3_Score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
