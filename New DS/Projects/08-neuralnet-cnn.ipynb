{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "randomprojects",
   "language": "python",
   "display_name": "RandomProjects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "name": "08-neuralnet-cnn.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1HWTVel3pBY"
   },
   "source": [
    "# Project 8: CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dE3SLyWQ3pBb"
   },
   "source": [
    "## Instructions\n",
    "\n",
    "### Description\n",
    "\n",
    "In this project you will be learning about Convolutional Neural Networks (CNN) and messing with the parameters to see how they can change the output of the CNN. You will start with some questions about Tensorflow, the python package used for CNN! Good luck!\n",
    "\n",
    "### Grading\n",
    "\n",
    "For grading purposes, we will clear all outputs from all your cells and then run them all from the top.  Please test your notebook in the same fashion before turning it in.\n",
    "\n",
    "### Submitting Your Solution\n",
    "\n",
    "To submit your notebook, first clear all the cells (this won't matter too much this time, but for larger data sets in the future, it will make the file smaller).  Then use the File->Download As->Notebook to obtain the notebook file.  Finally, submit the notebook file on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUWr69gS3pBb"
   },
   "source": [
    "\n",
    "## 40 points\n",
    "\n",
    "### Instructions\n",
    "- The **\"MNIST\" data set** is composed of 28x28 pixel, black and white images of handwritten digits (0-9). It is commonly used to demonstrate the training and testing of CNNs.\n",
    "- **We provide you with framework code** to build a CNN of roughly suitable size for the MNIST digit recognition task.\n",
    "\n",
    "\n",
    "- **Your tasks**:\n",
    "    1. Read the linked article and answer the questions under the \"What is Tensorflow\" section\n",
    "    2. **Execute the train/test code** and observe the results of the gradient descent training.\n",
    "    3. Complete all 4 modifications of the code, thee modification instructions are:\n",
    "         - **Modify the model architecture in the following ways**, and repeat the training and testing. Each modification should be made relative to the original network. Do not keep \"adding\" each modification with each new model (which would lead to a final model having all the modifications below):\n",
    "           - **Modification 1**: Remove the ReLU activation functions and the max-pooling layers, thereby making the entire network a linear function.\n",
    "           - **Modification 2**: Increase the size of the model by a factor of 16, roughly.\n",
    "           - **Modification 3**: Convert the large convolutional model to a large non-convolutional model, with roughly the same number of parameters.\n",
    "           - **Modification 4**: Train the original baseline model with a larger training data set. \n",
    "     4. MAKE SURE THAT: along the way, **answer the questions in the Jupyter notebook cells**. You can just do this with comments within the cell, for example...\n",
    "   - ```## Answer 1: The training loss increased when I did x, y, and z.```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QCYQem1i3pBc"
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import time\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !pip install tensorflow-macos\n",
    "# !pip install tensorflow-metal"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DhCDGsy3pBc"
   },
   "source": [
    "# Part 1\n",
    "## What is TensorFlow? (6 pts)\n",
    "\n",
    "Before we get started with the rest of the project, let's talk about what tensorflow is. You are required to read this article https://www.guru99.com/what-is-tensorflow.html, and answer the following questions:\n",
    "\n",
    "1. What are the three parts of the TensorFlow Architecture?\n",
    "The TensorFlow Library, The Execution Engine, and TensorBoard\n",
    "2. What is a Tensor and how does it represent data?\n",
    "A tensor is a generalized matrix that can be of any number of dimensions\n",
    "3. What is one advantage of the use of graphs in TensorFlow?\n",
    "One significant advantage of using computational graphs in TensorFlow is the separation of definition and execution of computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1Fo9v0U3pBd"
   },
   "source": [
    "# Part 2\n",
    "## Set TensorFlow verbosity level"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WoHe-pP33pBd"
   },
   "source": [
    "verbose = 2 # 0==no output, 1=accuracy/loss output, 2=progress bar output"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RXp_avK3pBd"
   },
   "source": [
    "## Create helper function to plot results of our model training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ofeKzDa03pBe"
   },
   "source": [
    "def plot_results(history):\n",
    "    epoch_num = np.arange(1, len(history.history['loss'])+1)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_num, history.history['loss'], label='training_loss')\n",
    "    plt.plot(epoch_num, history.history['val_loss'], label='test_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_num, history.history['accuracy'], label='training_accuracy')\n",
    "    plt.plot(epoch_num, history.history['val_accuracy'], label='test_accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iBWOpYg3pBe"
   },
   "source": [
    "## Load the MNIST data\n",
    "\n",
    "### You will not need to re-run or re-use (cut/paste) the three code cells below.\n",
    "\n",
    "The full dataset has 60,000 training images and 10,000 test images.\n",
    "\n",
    "To accelerate training on jupyterhub (with lots of users) we will only  \n",
    "use a subset of the training set. This will also let us explore some of the  \n",
    "hazards of training a neural net without a very large set of samples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ykQxHK9o3pBe",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1623263362505,
     "user_tz": 360,
     "elapsed": 546,
     "user": {
      "displayName": "Christopher Johnson",
      "photoUrl": "",
      "userId": "06055017626944874848"
     }
    },
    "outputId": "0f04cc0d-5811-4752-e2f4-e4f25a8605da"
   },
   "source": [
    "# Load data\n",
    "(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n",
    "\n",
    "# Use a subset of the full training and test sets for actual training and testing,\n",
    "# to accelerate training, and demonstrate possible pitfalls of smaller training data sets.\n",
    "\n",
    "n_train = 1000\n",
    "images_train = images_train[0:n_train,:,:]\n",
    "labels_train = labels_train[0:n_train]\n",
    "\n",
    "n_test = 1000\n",
    "images_test = images_test[0:n_test,:,:]\n",
    "labels_test = labels_test[0:n_train]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IclCPaMl3pBf"
   },
   "source": [
    "## Let's graph a few of the MNIST digits, to confirm that they look as expected\n",
    "\n",
    "Using plt.imshow(), graph the first sixteen numbers in the training images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FT8COBMl3pBf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1623263673305,
     "user_tz": 360,
     "elapsed": 1906,
     "user": {
      "displayName": "Christopher Johnson",
      "photoUrl": "",
      "userId": "06055017626944874848"
     }
    },
    "outputId": "e1ad2a86-2eaa-4096-a244-eebcf7fe4145"
   },
   "source": [
    "## You will not need to run this cell more than once, or cut/paste it elsewhere\n",
    "plt.figure(figsize=(8*2, 2*2))\n",
    "for i in range(16):\n",
    "    plt.subplot(2, 8, i+1)\n",
    "    plt.imshow(images_train[i,:,:], cmap='gray')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UfeGjXdl3pBf"
   },
   "source": [
    "# Create TensorFlow Dataset objects to hold train and test data.\n",
    "images_train = images_train/255 \n",
    "images_train = np.expand_dims(images_train, axis=3) # TensorFlow expects a channel dimension\n",
    "images_train = tf.cast(images_train, tf.float32)\n",
    "labels_train = tf.cast(labels_train, tf.float32)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "\n",
    "images_test = images_test/255\n",
    "images_test = np.expand_dims(images_test, axis=3) # TensorFlow expects a channel dimension\n",
    "images_test = tf.cast(images_test, tf.float32)\n",
    "labels_test = tf.cast(labels_test, tf.float32)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "dataset_train = dataset_train.cache()\n",
    "dataset_train = dataset_train.shuffle(n_train)\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.batch(batch_size)\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnOhRh5m3pBg"
   },
   "source": [
    "## Construct, compile, and train the baseline model. (4 pts)\n",
    "\n",
    "\n",
    "The code will plot the loss and accuracy scores that were collected during training. Remember that training uses gradient descent, so the model parameters are slowly updated as the model gets a closer and closer fit to the data. TensorFlow records the scores after each \"epoch\"--the number of iterations at which all samples in the training set have been used once, in the gradient descent process.\n",
    "\n",
    "We are now going to build teh model. In tensorflow, we build the model by first defining all the layers that the model is going to have, then we compile the model. Finally, we can train the model. We are going to define the model first. The rest is taken care of for you. \n",
    "\n",
    "We do this by calling `tf.keras.models.Sequential()` and inputting the list of layers, in order, as the parameter. Our baseline model is going to include two convalutional layers with pooling layers after each one, then we are going to have one dense layer with 64 neurons. It will then conclude with a dense layer of 10 that is our output layer. (Hint, you need to reduce the dimensionality before the data enters the neural network part of the model!)\n",
    "\n",
    "The following are names of layers that may be useful: \n",
    "\n",
    "  `tf.keras.layers.Conv2D()`\n",
    "\n",
    "  `tf.keras.layers.MaxPool2D()`\n",
    "\n",
    "  `tf.keras.layers.Flatten()`\n",
    "\n",
    "  `tf.keras.layers.Dense()`\n",
    "\n",
    "For our model, we want to make sure that our convolutional layers have a kernel size of 3x3, 4 kernels, and use the 'relu' activation. Our pooling layers are going to have a 2x2 pooling same and `padding='same'`.   Don't foget to add the relu activation to the dense layer with 64 neurons. \n",
    "\n",
    "More information about the layers can be found [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers)!\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "LmtrItii3pBg"
   },
   "source": [
    "## This is the baseline model. \n",
    "\n",
    "##Your Code goe here\n",
    "num_kernels = 4\n",
    "dense_layer_neurons = 64\n",
    "kernels_size = (3, 3)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(dense_layer_neurons, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),    \n",
    "    optimizer='adam',                # TODO: please note I am using a mac and the optimizer you selected isn't as fast so I am using a better version of adam. Original: optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQBQXyMS3pBg"
   },
   "source": [
    "## Part 2 Questions (6 pts)\n",
    "\n",
    "### Using the baseline model, answer the questions below"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k5Vf-v453pBh"
   },
   "source": [
    "##Assume one image is pushed through as the sole input, and asnwer with three dimensions.\n",
    "\n",
    "##(Hint: The default stride for MaxPool2D is 2, and the default stride for Conv2D is 1!)\n",
    "\n",
    "##Questions 1: What is the dimensionality of the input before it has gone through any layers?\n",
    "##Question 2: What is its dimensionality after passing through the first convulational layer?\n",
    "##Question 3: What is its dimensionality after passing through the first convulational layer and the first pooling layer?\n",
    "\n",
    "##Answer 1: \n",
    "## it takes an input of 28 by 28 by 1 the first two dimentions are for the x and y axis and the final dimension is for the color.\n",
    "##Answer 2: \n",
    "##26x26x4\n",
    "##Answer 3:\n",
    "##13x13x4"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQcKuuXa3pBh"
   },
   "source": [
    "## MODIFICATION 1 (6 pts)\n",
    "\n",
    "### Copy code from cell above, that builds and trains the baseline model, and plots the results.\n",
    "### Now alter the model as described below, and run the code.\n",
    "Alter the model by removing the ReLU and max-pooling non-linear activations.  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "IQz6T56a3pBi"
   },
   "source": [
    "## MODIFICATION 1\n",
    "## BELOW, PUT YOUR MODEL CONSTRUCTION, COMPILATION, AND FITTING CODE\n",
    "#%tensorboard --logdir logs\n",
    "num_kernels = 4\n",
    "dense_layer_neurons = 64\n",
    "kernels_size = (3, 3)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size),\n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(dense_layer_neurons),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "plot_results(history)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4dkGl--g3pBi"
   },
   "source": [
    "## MODIFICATION 1\n",
    "## QUESTION 1: During training, how did the loss (error) curves change from the baseline model to the linear (MOD 1)\n",
    "##             model, for both the training and test sets? What does this imply regarding underfitting or overfitting?\n",
    "## QUESTION 2: In general, did the ReLU and max-pooling non-linearities make for a better model or a worse model?\n",
    "\n",
    "## Answer 1:\n",
    "## it improves due to increased model capacity, but at the risk of clearly overfitting.\n",
    "## Answer 2:\n",
    "##yes there is significant overtraining as the is large divergence between training and validation loss."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KkqETXL3pBi"
   },
   "source": [
    "## MODIFICATION 2 (6 pts)\n",
    "\n",
    "### Copy code from the baseline model, for building, training, and plotting results.\n",
    "### Now alter the model as described below, and run the code.\n",
    "\n",
    "Make the network much larger, by:\n",
    "1. Increasing the number of kernels in the convolutional layers from 4 to 64 (16x).\n",
    "2. Increasing the number neurons in the dense layer from 64 to 1024 (16x)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "N8Gv4vnO3pBj"
   },
   "source": [
    "## MODIFICATION 2\n",
    "## BELOW, PUT YOUR MODEL CONSTRUCTION, COMPILATION, AND FITTING CODE\n",
    "##Your Code goe here\n",
    "num_kernels = 64\n",
    "dense_layer_neurons = 1024\n",
    "kernels_size = (3, 3)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(dense_layer_neurons, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5PK2re6g3pBj"
   },
   "source": [
    "## MODIFICATION 2\n",
    "## QUESTION 1: How did the performance of the larger model (MOD 2) compare to that of the baseline model?\n",
    "## QUESTION 2: Based on the training curves, does the larger model show any **clear** signs of overfitting,\n",
    "##             despite the large number of parameters?\n",
    "\n",
    "## Answer 1:\n",
    "## The convolutional model (MOD 2) performs better due to its ability to capture spatial relationships in the data.\n",
    "## Answer 2:\n",
    "##Yes, there is slight divergence between the training loss and the test loss which is a sign of overfitting but not nearly as much as the last model."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTglwvK03pBj"
   },
   "source": [
    "## MODIFICATION 3 (6 pts)\n",
    "\n",
    "### Copy code from the baseline model, for building, training, and plotting results.\n",
    "### Now alter the model as described below, and run the code.\n",
    "\n",
    "**Relative to the MOD 2 model**, remove the convolutional and max-pooling layers, replacing them with a single, new, dense layer.\n",
    "\n",
    "The two convolutional layers have 640 and 36,928 parameters, respectively, for a total of 37,586.\n",
    "For an input of 28x28 = 784 pixels (features), a dense layer with 48 neurons will have roughly 784x48 = 37,632 parameters. Thus, do the following:\n",
    "1. Remove the convolutional and max-pooling layers.\n",
    "2. After the Flatten() layer, add a new Dense layer with 48 neurons and a ReLU activation function.\n",
    "3. Keep the (now second) Dense layer of dense_layer_neurons==1024 neurons, and the final Dense layer of 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": false,
    "id": "2phSJLJ73pBj"
   },
   "source": [
    "## MODIFICATION 3\n",
    "## BELOW, PUT YOUR MODEL CONSTRUCTION, COMPILATION, AND FITTING CODE\n",
    "num_kernels = 4\n",
    "dense_layer_neurons = 1024\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # assuming the input image size is 28x28\n",
    "\n",
    "    tf.keras.layers.Dense(48, activation='relu'),\n",
    "    tf.keras.layers.Dense(dense_layer_neurons, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "plot_results(history)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qq4KE1tL3pBk"
   },
   "source": [
    "## MODIFICATION 3\n",
    "## QUESTION 1: How did the performance of the large non-convolutional model (MOD 3) compare to that of the large convolutional model (MOD 2)?\n",
    "## QUESTION 2: Speculate on why the convolutional model performs better or worse than the non-convolutional model. There is a \"right\" answer, but we're just looking for your opinion/guess. No penalty for error.\n",
    "\n",
    "## Answer 1:\n",
    "##  It did worse than the mod 2 model\n",
    "## Answer 2:\n",
    "##There are several reasons why it does better probably the main one is that due to the nature of a convolution model, translation does not affect the performance i.e. if the number is shifted, that does not affect its prediction quality. Also, a convolution model is much better at spatial structures than a non-convolution model."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhBUhsR03pBk"
   },
   "source": [
    "## MODIFICATION 4 (6 pts)\n",
    "\n",
    "## This model/data with take longer to train the the previous ones--upwards of several minutes.\n",
    "\n",
    "### For the modification, we don't modify the model, we modify the size of the data sets. The two cells below create training and testing sets with 10,000 samples each, whereas our previous models used only 1,000 samples each.\n",
    "\n",
    "### Copy code from the baseline model, for building, training, and plotting results.\n",
    "- Place the copied code after the two cells below.  \n",
    "- Run all the cells below to get the new data sets, and train the baseline model on that data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eHFU460w3pBk"
   },
   "source": [
    "# Load data\n",
    "(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n",
    "\n",
    "# Use a subset of the full training and test sets for actual training and testing,\n",
    "# to accelerate training, and demonstrate possible pitfalls of smaller training data sets.\n",
    "\n",
    "n_train = 10000\n",
    "images_train = images_train[0:n_train,:,:]\n",
    "labels_train = labels_train[0:n_train]\n",
    "\n",
    "n_test = 10000\n",
    "images_test = images_test[0:n_test,:,:]\n",
    "labels_test = labels_test[0:n_train]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-5barLrn3pBk"
   },
   "source": [
    "# Create TensorFlow Dataset objects to hold train and test data.\n",
    "images_train = images_train/255\n",
    "images_train = np.expand_dims(images_train, axis=3) # TensorFlow expects a channel dimension\n",
    "images_train = tf.cast(images_train, tf.float32)\n",
    "labels_train = tf.cast(labels_train, tf.float32)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    "\n",
    "images_test = images_test/255\n",
    "images_test = np.expand_dims(images_test, axis=3) # TensorFlow expects a channel dimension\n",
    "images_test = tf.cast(images_test, tf.float32)\n",
    "labels_test = tf.cast(labels_test, tf.float32)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((images_test, labels_test))\n",
    "\n",
    "dataset_train = dataset_train.cache()\n",
    "dataset_train = dataset_train.shuffle(n_train)\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.batch(batch_size)\n",
    "dataset_test = dataset_test.cache()\n",
    "dataset_test = dataset_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "czOw9yfw3pBk"
   },
   "source": [
    "## MODIFICATION 4\n",
    "## BELOW, PUT YOUR MODEL CONSTRUCTION, COMPILATION, AND FITTING CODE\n",
    "## This is the baseline model. \n",
    "\n",
    "##Your Code goe here\n",
    "num_kernels = 4\n",
    "dense_layer_neurons = 64\n",
    "kernels_size = (3, 3)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Conv2D(num_kernels, kernels_size, activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(dense_layer_neurons, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Do not change any arguments in the call to model.compile()\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Do not change any arguments in the call to model.fit()\n",
    "epochs = 30\n",
    "t = time.time()\n",
    "history = model.fit(dataset_train,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=dataset_test,\n",
    "                    verbose=verbose)\n",
    "print('Training duration: %f seconds.' % (time.time() - t))\n",
    "\n",
    "# Plot results\n",
    "plot_results(history)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ulZPMS_j3pBl"
   },
   "source": [
    "## MODIFICATION 4\n",
    "## QUESTION 1: How did the performance of the baseline model trained on the larger data set (MOD 4) compare to that trained on the smaller data set?\n",
    "## QUESTION 2: This is just guess on your part... how much better do you think results might be if you trained the model on all 60,000 training samples (rather than 1000 or 10,000)?\n",
    "\n",
    "## Answer 1:\n",
    "## It did about 8% better training on the larger model over the smaller model\n",
    "## Answer 2:\n",
    "## I believe it is probably diminishing returns at that point, and won't do much better than the model trained currently."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkAtboF-3pBm"
   },
   "source": [
    "### Questionnaire\n",
    "1) How long did you spend on this assignment?\n",
    "<br>This took about two hours.<br>\n",
    "2) What did you like about it? What did you not like about it?\n",
    "<br>I like working with no networks, so this was very fun for me<br>\n",
    "3) Did you find any errors or is there anything you would like changed?\n",
    "<br>N/A<br>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TQYUhMqS3pBm"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}
